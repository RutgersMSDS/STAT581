library(text2vec)#import text2vec
library(data.table)
data("movie_review")#import data as data frame
setDT(movie_review)
setkey(movie_review,id)#set movie review as database, id is key
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
movie_review$review_clean = prep_fun(movie_review$review)
install.packages("stringr")
library(stringr)
movie_review$review_clean = prep_fun(movie_review$review)
View(movie_review)
movie_review$review = prep_fun(movie_review$review)
all_ids=movie_review$id#get all review id
train_ids = sample(all_ids, 1000)#take randomly 1000 id
test_ids = setdiff(all_ids, train_ids)#rest id
training = movie_review[train_ids,]#take part 1 data, 1000 datapoint
testing = movie_review[test_ids,]#part 2 data, 4000 datapoint
traintok<-itoken(training$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")#set "useless" word
vocab = create_vocabulary(traintok, stopwords = stop_words)#remove useless word
pruned_vocab = prune_vocabulary(vocab,term_count_min=1,doc_proportion_max=0.5, doc_proportion_min=0.001)#pruned low frequncy words
vectorizer = vocab_vectorizer(pruned_vocab)#word to vector
dtm_train = create_dtm(traintok, vectorizer)#create dtm for part 1 data
testtok<-itoken(testing$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
dtm_test  = create_dtm(testtok, vectorizer)#create dtm for part 2 data
d1_d2_cos_sim = sim2(dtm_train, dtm_test, method = "cosine", norm = "l2")#create cosine distance matrix
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[1:5] #first five data relevent to train[1] data
View(test_ids)
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-5:-1] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-1:-5] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE) #first five data relevent to train[1] data
View(t1cor)
library(text2vec)#import text2vec
library(data.table)
library(stringr)
library(stringi)
trainq<- readRDS(file = "questions.rds")
getwd()
setwd("V:/STAT581/581Project Final")
trainq<- readRDS(file = "questions.rds")
all_facts<-read.csv("all_facts.csv",header=TRUE)
names(trainq)<-c("ques","explain")
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
#tokenize question/fact
factrim=prep_fun(all_facts$fact)
View(factrim)
facttok<-itoken(factrim, preprocessor = tolower, tokenizer = word_tokenizer, progressbar = TRUE)
quetrim=prep_fun(trainq$ques)
quetok<-itoken(quetrim, preprocessor = tolower, tokenizer = word_tokenizer, progressbar = TRUE)
#make stop word list
stop_words = c("which", "why","for","its", "what", "where", "of", "as", "how", "is", "a", "that","this", "these","to", "he", "she", "are","in","on","and","an","be","by","it")
vocab = create_vocabulary(quetok, stopwords = stop_words)
# Remove words too frequent
pruned_vocab = prune_vocabulary(vocab,term_count_min=1,doc_proportion_max=0.5)
#Create dictionary
vectorizer = vocab_vectorizer(pruned_vocab)
#Create DTM
dtm_qes = create_dtm(quetok, vectorizer)
View(dtm_qes)
View(pruned_vocab)
dtm_fact  = create_dtm(facttok, vectorizer)
#calculate cosine distance
simmat = sim2(dtm_qes, dtm_fact, method = "cosine", norm = "l2")
View(simmat)
lensim=nrow(simmat)#get length of questions
res<-data.frame()
lenfact=nrow(all_facts)
i=1
nexist=0
nrexist=0
exist=0
while (i<=lensim){
ord=order(simmat[i,],decreasing = TRUE)
tcor=ord[1:5]
random=sample(ord,size=5)
ans=paste(all_facts$fact[tcor],sep = ",")
exid=all_facts$id[tcor]#get top five explains of each question
ranid=all_facts$id[random]#get random five explains of each question
#get how many finding facts are true correspond facts
find=stri_count_regex(trainq$explain[i,], paste(exid, collapse="|"))
findr=stri_count_regex(trainq$explain[i,], paste(ranid, collapse="|"))
res<-rbind(res,c(trainq$ques[i],ans))
exist=exist+find
#see if this question's fact did not to be found
if (find==0){
nexist=nexist+1
}
if (findr==0){
nrexist=nrexist+1
}
i=i+1
}
#show result
cat("not find rate:",nexist/lensim)
cat("random not find rate:",nrexist/lensim)
cat("average each question find:",exist/lensim)
