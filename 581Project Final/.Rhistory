library(text2vec)#import text2vec
library(data.table)
data("movie_review")#import data as data frame
setDT(movie_review)
setkey(movie_review,id)#set movie review as database, id is key
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
movie_review$review_clean = prep_fun(movie_review$review)
install.packages("stringr")
library(stringr)
movie_review$review_clean = prep_fun(movie_review$review)
View(movie_review)
movie_review$review = prep_fun(movie_review$review)
all_ids=movie_review$id#get all review id
train_ids = sample(all_ids, 1000)#take randomly 1000 id
test_ids = setdiff(all_ids, train_ids)#rest id
training = movie_review[train_ids,]#take part 1 data, 1000 datapoint
testing = movie_review[test_ids,]#part 2 data, 4000 datapoint
traintok<-itoken(training$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")#set "useless" word
vocab = create_vocabulary(traintok, stopwords = stop_words)#remove useless word
pruned_vocab = prune_vocabulary(vocab,term_count_min=1,doc_proportion_max=0.5, doc_proportion_min=0.001)#pruned low frequncy words
vectorizer = vocab_vectorizer(pruned_vocab)#word to vector
dtm_train = create_dtm(traintok, vectorizer)#create dtm for part 1 data
testtok<-itoken(testing$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
dtm_test  = create_dtm(testtok, vectorizer)#create dtm for part 2 data
d1_d2_cos_sim = sim2(dtm_train, dtm_test, method = "cosine", norm = "l2")#create cosine distance matrix
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[1:5] #first five data relevent to train[1] data
View(test_ids)
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-5:-1] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-1:-5] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE) #first five data relevent to train[1] data
View(t1cor)
getwd()
setwd("V:\STAT581\581Project Final")
setwd("V:/STAT581/581Project Final")
library(tidyr)
# get all file names in tables folder
filenames <- list.files("tables", pattern="*.tsv", full.names=TRUE)
paste(filenames)
# read each file in tables folder as a dataframe
datalist = list()
for (i in 1:length(filenames)) {
df <- read.delim(filenames[i], header=FALSE, sep="\t")
# remove first row (header)
df_without_header <- df[-1,]
# remove last 3 columns except id
df_trimmed <- df_without_header[, -c(length(df_without_header)-3, length(df_without_header)-2)]
# concat all columns except last as one
df_fact <- unite(df_trimmed[, -c(length(df_trimmed)-1)], 'fact', colnames(df_trimmed[, -c(length(df_trimmed)-1)]), sep = " ", remove = TRUE, na.rm = TRUE)
# trim leading and trailing spaces
df_fact_tidy <- data.frame(lapply(df_fact, trimws), stringsAsFactors = FALSE)
# add the id column
df_fact_tidy$id <- df_trimmed[, c(length(df_trimmed)-1)]
datalist[[i]] <- df_fact_tidy
}
# merge all dataframes into one
all_facts = do.call(rbind, datalist)
# export output
write.table(all_facts, file = "all_facts.csv", row.names = FALSE, col.names = TRUE,sep=",")
library(tidyr)
library(dplyr)
library(stringr)
# function to extract correct answer string from the question column using the correct answer key
getCorrectAns <- function(quesString, key)
{
keyString <- paste("\\(", key ,"\\)", sep="")
spl1 <- strsplit(sub(keyString, "START", quesString), "START", fixed = TRUE)
res1 <- sapply(spl1, "[", 2)
spl2 <- strsplit(sub("\\(", "END", res1), "END", fixed = TRUE)
res2 <- sapply(spl2, "[", 1)
res3 <- gsub("[()]", "", res2)
ans <- str_trim(res3)
return(ans)
}
# function to extract the question statement without the answer choices from the question column
getQues <- function(quesString)
{
endchar <- paste("\\(", "A" ,"\\)", sep="")
spl <- strsplit(sub(endchar, "END", quesString), "END", fixed = TRUE)
res <- sapply(spl, "[", 1)
ques <- str_trim(res)
return(ques)
}
# function to perform data preprocessing on a raw questions file
questionsPreprocessing <- function(split)
{
# read questions file
inputFile <- paste("questions/questions", ".", split, ".tsv", sep="")
df <- read.delim(inputFile, header=TRUE, sep="\t")
df_raw <- select(df, 'question', 'AnswerKey')
# extract correct answer string
getCorrectAns_v <- Vectorize(getCorrectAns)
(df_raw$ans <- getCorrectAns_v(df_raw$question, df_raw$AnswerKey))
# extract question statement
getQues_v <- Vectorize(getQues)
(df_raw$ques <- getQues_v(df_raw$question))
# concat question and correct answer strings
df_ques_ans <- select(df_raw, 'ques', 'ans')
df_out <- unite(df_ques_ans, 'merged_ques_ans', colnames(df_ques_ans), sep = " ", remove = TRUE, na.rm = TRUE)
# trim leading and trailing spaces
df_out_tidy <- data.frame(lapply(df_out, trimws), stringsAsFactors = FALSE)
# add correct explanation column
df_out_tidy$explanation <- select(df, 'explanation')
print(df_out_tidy)
# export output
outputFile <- paste("questions", ".", split, ".rda", sep="")
saveRDS(object=df_out_tidy,file="questions.rds")
return (df_out_tidy)
}
tidy=questionsPreprocessing("train")
library(text2vec)#import text2vec
library(data.table)
library(stringr)
library(stringi)
trainq<- readRDS(file = "questions.rds")
all_facts<-read.csv("all_facts.csv",header=TRUE)
names(trainq)<-c("ques","explain")
a=trainq$explain
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
#tokenize question/fact
factrim=prep_fun(all_facts$fact)
facttok<-itoken(factrim, preprocessor = tolower, tokenizer = word_tokenizer, progressbar = TRUE)
quetrim=prep_fun(trainq$ques)
quetok<-itoken(quetrim, preprocessor = tolower, tokenizer = word_tokenizer, progressbar = TRUE)
#make stop word list
stop_words = c("which", "why","for","its", "what", "where", "of", "as", "how", "is", "a", "that","this", "these","to", "he", "she", "are","in","on","and","an","be","by","it")
vocab = create_vocabulary(quetok, stopwords = stop_words)
# Remove words too frequent
pruned_vocab = prune_vocabulary(vocab,term_count_min=1,doc_proportion_max=0.5)
#Create dictionary
vectorizer = vocab_vectorizer(pruned_vocab)
#Create DTM
dtm_qes = create_dtm(quetok, vectorizer)
dtm_fact  = create_dtm(facttok, vectorizer)
#calculate cosine distance
simmat = sim2(dtm_qes, dtm_fact, method = "cosine", norm = "l2")
lensim=nrow(simmat)#get length of questions
res<-data.frame()
lenfact=nrow(all_facts)
i=1
nexist=0
nrexist=0
exist=0
while (i<=lensim){
ord=order(simmat[i,],decreasing = TRUE)
tcor=ord[1:5]
random=sample(ord,size=5)
ans=paste(all_facts$fact[tcor],sep = ",")
exid=all_facts$id[tcor]#get top five explains of each question
ranid=all_facts$id[random]#get random five explains of each question
#get how many finding facts are true correspond facts
find=stri_count_regex(trainq$explain[i,], paste(exid, collapse="|"))
findr=stri_count_regex(trainq$explain[i,], paste(ranid, collapse="|"))
res<-rbind(res,c(trainq$ques[i],ans))
exist=exist+find
#see if this question's fact did not to be found
if (find==0){
nexist=nexist+1
}
if (findr==0){
nrexist=nrexist+1
}
i=i+1
}
#show result
cat("not find rate:",nexist/lensim)
cat("random not find rate:",nrexist/lensim)
cat("average each question find:",exist/lensim)
#write table
write.table(res, file = "q&a.csv", row.names = FALSE, col.names = FALSE,sep=",")
