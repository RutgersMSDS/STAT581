library(text2vec)#import text2vec
library(data.table)
data("movie_review")#import data as data frame
setDT(movie_review)
setkey(movie_review,id)#set movie review as database, id is key
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
movie_review$review_clean = prep_fun(movie_review$review)
install.packages("stringr")
library(stringr)
movie_review$review_clean = prep_fun(movie_review$review)
View(movie_review)
movie_review$review = prep_fun(movie_review$review)
all_ids=movie_review$id#get all review id
train_ids = sample(all_ids, 1000)#take randomly 1000 id
test_ids = setdiff(all_ids, train_ids)#rest id
training = movie_review[train_ids,]#take part 1 data, 1000 datapoint
testing = movie_review[test_ids,]#part 2 data, 4000 datapoint
traintok<-itoken(training$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")#set "useless" word
vocab = create_vocabulary(traintok, stopwords = stop_words)#remove useless word
pruned_vocab = prune_vocabulary(vocab,term_count_min=1,doc_proportion_max=0.5, doc_proportion_min=0.001)#pruned low frequncy words
vectorizer = vocab_vectorizer(pruned_vocab)#word to vector
dtm_train = create_dtm(traintok, vectorizer)#create dtm for part 1 data
testtok<-itoken(testing$review, preprocessor = tolower, tokenizer = word_tokenizer, ids = training$id, progressbar = TRUE)#function to lower all character, split as word
dtm_test  = create_dtm(testtok, vectorizer)#create dtm for part 2 data
d1_d2_cos_sim = sim2(dtm_train, dtm_test, method = "cosine", norm = "l2")#create cosine distance matrix
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[1:5] #first five data relevent to train[1] data
View(test_ids)
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-5:-1] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE)[-1:-5] #first five data relevent to train[1] data
t1cor=order(d1_d2_cos_sim[1,],decreasing=TRUE) #first five data relevent to train[1] data
View(t1cor)
library(tidyr)
install.packages("tidyr")
library(tidyr)
# get all file names in tables folder
filenames <- list.files("tables", pattern="*.tsv", full.names=TRUE)
# get all file names in tables folder
filenames <- list.files("tables", pattern="*.tsv", full.names=TRUE)
paste(filenames)
getwd()
setwd("V:/STAT581")
# get all file names in tables folder
filenames <- list.files("./tables", pattern="*.tsv", full.names=TRUE)
# get all file names in tables folder
filenames <- list.files("./581Project/facts/tables", pattern="*.tsv", full.names=TRUE)
View(filenames)
# read each file in tables folder as a dataframe
datalist = list()
for (i in 1:length(filenames)) {
df <- read.delim(filenames[i], header=FALSE, sep="\t")
# remove first row (header)
df_without_header <- df[-1,]
# remove last 3 columns
df_trimmed <- df_without_header[1:(length(df_without_header)-4)]
# concat all columns as one
df_fact <- unite(df_trimmed, 'fact', colnames(df_trimmed), sep = " ", remove = TRUE, na.rm = TRUE)
# trim leading and trailing spaces
df_fact_tidy <- data.frame(lapply(df_fact, trimws), stringsAsFactors = FALSE)
datalist[[i]] = df_fact_tidy
}
# merge all dataframes into one
all_facts = do.call(rbind, datalist)
View(all_facts)
View(datalist)
View(df_fact)
View(df_fact_tidy)
View(df_trimmed)
View(df_without_header)
library(text2vec)#import text2vec
library(data.table)
library(stringr)
library(tidyr)
# get all file names in tables folder
filenames <- list.files("./581Project/facts/tables", pattern="*.tsv", full.names=TRUE)
# read each file in tables folder as a dataframe
datalist = list()
for (i in 1:length(filenames)) {
df <- read.delim(filenames[i], header=FALSE, sep="\t")
# remove first row (header)
df_without_header <- df[-1,]
# remove last 3 columns
df_trimmed <- df_without_header[1:(length(df_without_header)-4)]
# concat all columns as one
df_fact <- unite(df_trimmed, 'fact', colnames(df_trimmed), sep = " ", remove = TRUE, na.rm = TRUE)
# trim leading and trailing spaces
df_fact_tidy <- data.frame(lapply(df_fact, trimws), stringsAsFactors = FALSE)
datalist[[i]] = df_fact_tidy
}
# merge all dataframes into one
all_facts = do.call(rbind, datalist)
prep_fun=function(x){
x = str_to_lower(x)
# remove non-alphanumeric symbols
x = str_replace_all(x, "[^[:alnum:]]", " ")
# collapse multiple spaces
str_replace_all(x, "\\s+", " ")
}
factrim=prep_fun(all_facts$fact)
facttok<-itoken(factrim, preprocessor = tolower, tokenizer = word_tokenizer, progressbar = TRUE)
